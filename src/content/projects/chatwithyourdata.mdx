---
author: Mihai-Cristian Farca»ô
title: Chat With Your Data
date: 2025-02-07
image: /images/projects/chatwithyourdata.jpeg
summary: LLM chatbot for your personal data.
---

## _PDF Chatbot using LLM and Streamlit_

This post is about taking a new look at your personal data through a chatbot that can answer your questions based on the PDF files you give it. Pretty cool, right?

The chatbot works on **CPU** and uses **Retrieval-Augmented Generation (RAG)** to answer questions based on information stored in **PDF documents**. This is achieved using **LangChain** for document processing and retrieval, **FAISS** for vector storage, and **Hugging Face's transformers library** for language model processing.

## _Features_

- **Interactive Q&A**: Ask questions related to the contents of PDF files, and get answers generated by an LLM.
- **PDF Document Retrieval**: PDFs stored in the `docs` directory are loaded and processed for easy access during chats.
- **Conversational Memory** _(To be implemented)_: The chatbot maintains a chat history to provide contextually relevant responses within the conversation.
- **Streamlit Chat UI**: Simple, intuitive interface using Streamlit, supporting conversation-based interaction with your PDF data.

## _Requirements_

- Python 3.8 or higher
- Install the dependencies listed in `requirements.txt` using

```bash
pip install -r requirements.txt
```

If you are working with the notebook, running the first block is sufficient.
- 16GB of RAM

## _How to use the Chatbot for your data_

### 1. Setup PDF Data

Add the PDF files to a `docs` directory. The chatbot will load these PDFs, process the text, and create a vector store for retrieval. In the notebook example, I loaded and processed my results report from the Understand Myself personality test.

### 2. Run the Application

To start the chatbot:

```bash
streamlit run chatbot.py
```

### 3. Ask Questions

Use the text input field to ask questions about the data within your PDF documents. The chatbot will retrieve relevant information and generate answers based on the content of your PDFs.

## _Code Overview_

- **Model Loading**: The LaMini-T5-738M model is loaded from Hugging Face as a text2text-generation pipeline.
- **PDF Loading and Text Splitting**: PDFs are processed using the PyPDFLoader and split into smaller chunks with RecursiveCharacterTextSplitter.
- **Vector Store Creation**: Text chunks are converted into embeddings and stored in a FAISS vector store for efficient retrieval.
- **Question-Answering Chain**: A Conversational Retrieval Chain combines LLM responses with retrieved content to provide informed answers.
- **Streamlit Interface**: The chatbot UI displays past user inputs and generated responses.

You can find the [source code](https://github.com/Forquosh/LaMiniT5-Langchain-FAISS-RAG.git) on my Github.
