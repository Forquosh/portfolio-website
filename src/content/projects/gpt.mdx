---
author: Mihai-Cristian Farca»ô
title: Generative Pretrained Transformer
date: 2025-02-11
image: /images/projects/gpt.jpg
source: https://cdn.the-scientist.com/assets/articleNo/71687/aImg/52292/62dc0501-8dda-4bd7-9ba9-fa1a9b8c7cb4-l.jpg
summary: GPT-like model implementation built from scratch using PyTorch.
---

## _My PyTorch implementation of a GPT-like language model with text preprocessing utilities_

## _Overview_

This project implements a transformer-based language model similar to GPT, designed for **character-level text generation**. It includes utilities for vocabulary generation and dataset splitting.

In this example, I trained and tested it on the fabulous book **[The Brothers Karamazov](https://www.gutenberg.org/ebooks/28054)**, downloaded from **Project Gutenberg**. Feel free to change the text file or even try training it on an consacrated dataset (like **OpenWebText**, for example), though on larger datasets the `vocab.py` and `split.py` might not work properly.

## _Features_

- Character-level language modeling
- Multi-head self-attention mechanism
- Memory-efficient data loading using memory mapping
- Text preprocessing utilities
- Configurable model architecture

## _Requirements_

- Python 3.9+
- PyTorch
- Jupyter Notebooks
- CUDA (optional, for GPU acceleration, **on Windows**)

## _Project Structure_

- `vocab.py` - Generates vocabulary from input text
- `split.py` - Splits text data into training and validation sets
- `GPT.ipynb` - Main model implementation and training

## _Usage_

### 1. Initialization (details on [my Github](https://github.com/Forquosh/GPT.git))

### 2. Prepare Your Data

- First, add your desired data file and generate the vocabulary from your text:

```bash
python3 vocab.py
```

- Then, split your data into training and validation sets:

```bash
python3 split.py
```

### 3. Train the Model

- Install a new kernel to use in your Jupyter Notebook:

```bash
python3 -m ipykernel install --user --name=venv --display-name "GPTKernel"
```

- Run Jupyter Notebook:

```bash
jupyter notebook
```

- Open `GPT.ipynb`.

- Select `GPTKernel` and run the cells sequentially.

## _The notebook contains_

- Model architecture implementation
- Training loop
- Text generation functionality

## _Model Architecture_

The model implements a transformer architecture with:

- Multi-head self-attention
- Position embeddings
- Layer normalization
- Feed-forward networks

You can find the [source code](https://github.com/Forquosh/GPT.git) on my Github.

<ContentFooter />
